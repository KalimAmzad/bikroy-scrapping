{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kalim\\anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time \n",
    "from requests_html import HTMLSession\n",
    "# from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "from PIL import Image\n",
    "from io import StringIO, BytesIO\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from IPython.display import display, clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.1.2-py3-none-any.whl (963 kB)\n",
      "Collecting urllib3[secure,socks]~=1.26\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi; extra == \"secure\" in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0.0; extra == \"secure\" in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: pyOpenSSL>=0.14; extra == \"secure\" in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (19.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=1.3.4; extra == \"secure\" in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: PySocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: async-generator>=1.10 in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.10)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied, skipping upgrade: sortedcontainers in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.2.2)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.8 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied, skipping upgrade: attrs>=19.2.0 in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.2 in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14; extra == \"secure\"->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in c:\\users\\kalim\\anaconda3\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.20)\n",
      "Installing collected packages: urllib3, sniffio, outcome, trio, h11, wsproto, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 3.9.0\n",
      "    Uninstalling selenium-3.9.0:\n",
      "      Successfully uninstalled selenium-3.9.0\n",
      "Successfully installed h11-0.13.0 outcome-1.1.0 selenium-4.1.2 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bikroy.com/en/ads/bangladesh/mobiles?sort=date&order=desc&buy_now=0&urgent=0&page=100\n",
      "Page: 100 URL: 2242\n",
      "Page: 100 URL: 2243\n",
      "Page: 100 URL: 2244\n",
      "Page: 100 URL: 2245\n",
      "Page: 100 URL: 2246\n",
      "Page: 100 URL: 2247\n",
      "Page: 100 URL: 2248\n",
      "Page: 100 URL: 2249\n",
      "Page: 100 URL: 2250\n",
      "Page: 100 URL: 2251\n",
      "Page: 100 URL: 2252\n",
      "Page: 100 URL: 2253\n",
      "Page: 100 URL: 2254\n",
      "Page: 100 URL: 2255\n",
      "Page: 100 URL: 2256\n",
      "Page: 100 URL: 2257\n",
      "Page: 100 URL: 2258\n",
      "Page: 100 URL: 2259\n",
      "Page: 100 URL: 2260\n",
      "Page: 100 URL: 2261\n",
      "Page: 100 URL: 2262\n",
      "Page: 100 URL: 2263\n",
      "Page: 100 URL: 2264\n",
      "Page: 100 URL: 2265\n",
      "Page: 100 URL: 2266\n",
      "Page: 100 URL: 2267\n",
      "Page: 100 URL: 2268\n"
     ]
    }
   ],
   "source": [
    "s = Service('chromedriver')\n",
    "driver = webdriver.Chrome(service=s)\n",
    "pages = 100\n",
    "single_page_urls = []\n",
    "page_cnt, url_cnt = 0, 0\n",
    "for p in range(1, pages+1):\n",
    "    bikroy_url = f'https://bikroy.com/en/ads/bangladesh/mobiles?sort=date&order=desc&buy_now=0&urgent=0&page={p}'\n",
    "    print(bikroy_url)\n",
    "    driver.get(bikroy_url)\n",
    "    time.sleep(3)\n",
    "    page_cnt += 1\n",
    "    for i in range (1, 28):\n",
    "        try:\n",
    "            single_url = driver.find_element(by=By.XPATH, value=f'//*[@id=\"app-wrapper\"]/div[1]/div[3]/div/div[2]/div[4]/div[2]/div/div[1]/div[1]/ul/li[{i}]/a').get_attribute('href')\n",
    "            single_page_urls.append(single_url)\n",
    "            url_cnt += 1\n",
    "            print(\"Page: \" + str(page_cnt) + \" URL: \" + str(url_cnt))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "# driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2268"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_page_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mobile_product_urls.txt', 'w') as f:\n",
    "    for s in single_page_urls:\n",
    "        f.write(s)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_scrap(driver):\n",
    "    title = driver.find_element(by=By.TAG_NAME, value='h1').text\n",
    "    #     //*[@id=\"app-wrapper\"]/div[1]/div[2]/div[2]/div[1]/div[1]/div/div[1]/div/div[1]/div[2]/div/div/h1\n",
    "    #     //*[@id=\"app-wrapper\"]/div[1]/div[2]/div[2]/div[1]/div[1]/div/div[1]/div/div[1]/div[2]/div/div/h1\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_scarp(driver):\n",
    "    try:\n",
    "        images = driver.find_element(by=By.XPATH, value='//*[@id=\"app-wrapper\"]/div[1]/div[2]/div[2]/div[3]/div[2]/div/div[1]/div/div[1]/div/div/ul')\n",
    "        img_path = []\n",
    "        for img in images.find_elements(by=By.TAG_NAME,value='img'):\n",
    "            img_urls = img.get_attribute('src')\n",
    "\n",
    "            img_urls_firstname = '/'.join(img_urls.split('/')[:-3])\n",
    "            img_urls_rename = img_urls_firstname + '/620/466/fitted.jpg'\n",
    "    #         print(img_urls_rename)\n",
    "            img_name = '@'.join(img_urls_firstname.split('/')[-2:])\n",
    "\n",
    "            r = requests.get(img_urls_rename)\n",
    "            i = Image.open(BytesIO(r.content))\n",
    "            i.save(f\"img/mobile/{img_name}.jpg\")\n",
    "            img_path.append(img_name)\n",
    "        return img_path\n",
    "    except:\n",
    "        print('No images')        \n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_data_scrap(driver):\n",
    "    try:\n",
    "        meta_data = driver.find_element(by=By.XPATH, value='//*[@id=\"app-wrapper\"]/div[1]/div[2]/div[2]/div[3]/div[2]/div/div[1]/div/div[2]/div[2]')\n",
    "\n",
    "        md = meta_data.find_elements(by=By.TAG_NAME,value='div')\n",
    "        meta_dic = {}\n",
    "        for m in md:\n",
    "            m = m.text\n",
    "            if '\\n' in m:\n",
    "                key, val = m.split('\\n')[0].replace(':', ''), m.split('\\n')[1]\n",
    "                meta_dic[key] = val\n",
    "        print(\"Meta Data: \", meta_dic)\n",
    "        return meta_dic\n",
    "    except:\n",
    "        print('No meta data found')        \n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_more_click(driver):    \n",
    "    try:\n",
    "        s_driver.find_element(by=By.CLASS_NAME, value='show-text--13FcL').click()\n",
    "#         s_driver.find_element(by=By.XPATH, value='//*[@id=\"app-wrapper\"]/div[1]/div[2]/div[2]/div[3]/div[2]/div/div[1]/div/div[2]/div[3]/div/div[2]/div/div[2]/button').click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"No 'Show more' button\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scrap(driver):\n",
    "    try:\n",
    "        feature = driver.find_element(by=By.CLASS_NAME, value='text--Xap4O').text\n",
    "#         feature = s_driver.find_element(by=By.XPATH, value='//*[@id=\"app-wrapper\"]/div[1]/div[2]/div[2]/div[1]/div[1]/div/div[1]/div/div[2]/div[4]/p').text\n",
    "#         feature = feature.find_elements(by=By.TAG_NAME,value='p').text\n",
    "        print('Features: ', feature)\n",
    "        return feature\n",
    "    except:\n",
    "        print(\"No feature\")        \n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_scrap(driver):\n",
    "    # desc = s_driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[2]/div[2]/div[3]/div[2]/div/div[1]/div/div[2]/div[3]/div/div[2]/div/div[1]/div/ul/div')\n",
    "    # description[0].text\n",
    "    # description = [d.text.replace(\"'\\uf076\", \"\").replace(\"''\\uf0d8\", \"\") for d in desc.find_elements_by_tag_name('p')]\n",
    "    # print(description)\n",
    "\n",
    "    try:\n",
    "        desc = s_driver.find_element(by=By.XPATH, value='//*[@id=\"collapsible-content-0\"]/ul/div').text\n",
    "        print(\"Description: \", desc)\n",
    "        return desc\n",
    "    except:\n",
    "        print(\"No description\")        \n",
    "        return 'N/A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n"
     ]
    }
   ],
   "source": [
    "op = webdriver.ChromeOptions()\n",
    "op.add_argument('headless')\n",
    "s = Service('chromedriver')\n",
    "s_driver = webdriver.Chrome(service=s, options=op)\n",
    "\n",
    "cnt = 1\n",
    "titles, img_lst, meta_lst,  feature_lst, desc_lst = [[]]*5 \n",
    "for s_u in single_page_urls[:3]:\n",
    "    print(\"Count: \", cnt)\n",
    "    s_driver.get(s_u)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # single page data scrapping\n",
    "    s_title = title_scrap(s_driver)\n",
    "    print(\"Title: \", s_title)\n",
    "    titles.append(s_title)\n",
    "\n",
    "    # scrapping all images\n",
    "    images = image_scrap(s_driver)\n",
    "    img_lst.append(images)\n",
    "    \n",
    "\n",
    "    # extractig meta data\n",
    "    meta_data = meta_data_scrap(s_driver)\n",
    "    meta_lst.append(meta_data)\n",
    "        \n",
    "    # click on show_more button\n",
    "    show_more_click(s_driver)\n",
    "\n",
    "    # extracting feature data\n",
    "    feature = feature_scrap(s_driver)\n",
    "    feature_lst.append(feature)\n",
    "    \n",
    "    # extracting description\n",
    "    description = description_srcap(s_driver)\n",
    "    desc_lst.append(description)\n",
    "    \n",
    "    cnt += 1\n",
    "    print(\"=============================================\")\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lst = [titles, img//g_lst, meta_lst,  feature_lst, desc_lst]\n",
    "column_lst = ['Title', 'Images', 'Meta Data', 'Feature', 'Description']\n",
    "df = pd.DataFrame (df_lst, columns = column_lst)\n",
    "df.to_csv('Bikroy-Mobile-Category.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
